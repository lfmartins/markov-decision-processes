{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lfmartins/markov-decision-processes/blob/main/markov_decision_processes_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an example of MDP, let's consider a robot that moves in a $4\\times4$ grid. The objective is to mimimize the number of steps the robot takes to reach the one of the target cells $(0,0)$ and $(3,3)$. \n",
        "\n",
        "If the robot is in grid cell $(i,j)$, there are at most 4 actions, corresponding to the robot moving north, east, south and west (if the robot is on one of the edges not all actions are allowed). The reward of taking any one of the actions is $-1$, except at the target cells, in which case the reward is zero. \n",
        "\n",
        "A discrete MDP is defined in terms of two functions:\n",
        "\n",
        "- $p(s' \\mid s, a)$ is the probability that the process will be in state $s'$ at time $t$, given that the process is in sate $s$ at time $t-1$ and action $a$ is selected. (The notation $p(s' \\mid s,a)$ is used to resemble the notation for conditional probability, but it just represents a function of three variables, $s'$, $s$ and $a$.\n",
        "- $r(s,a)$ is the reward obtained for selecting action $a$.\n",
        "\n",
        "A *randomized policy* is represented by function of two variables $\\pi(a|s)$ that, to every action $a$ available in state $s$, associates the probability that action $a$ is chosen when in state $s$.\n",
        "\n",
        "To represent a MDP in Python we define the following data structures.\n",
        "\n",
        "A MDP is represented by objects of the class `MDP`. This is a very thin implementation. A `MDP` holds information about a set of states. States are created by the method `add_state()`. When a state is added, the only information that is recorded is the `state_id`. This means that just adding the states does not define the structure of the chain.\n",
        "\n",
        "To each state, we associate a set of actions through the `add_action()` method. An action specifies a reward and a set of transition probabilities."
      ],
      "metadata": {
        "id": "ekGXJ8ufNLFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "#from collections import defaultdict\n",
        "from random import choice\n",
        "\n",
        "class MDP(object):\n",
        "  def __init__(self):\n",
        "    self.states = dict()\n",
        "    self.states_list = []\n",
        "    self.terminal_states = set()\n",
        "    self.transition_probs = dict()\n",
        "    self.rewards = dict()\n",
        "  \n",
        "  def add_state(self, state_id, terminal=False):\n",
        "    if state_id in self.states:\n",
        "      raise ValueError(f'state {state_id} already exists')\n",
        "    self.states[state_id] = len(self.states_list)\n",
        "    self.states_list.append(state_id)\n",
        "    self.transition_probs[state_id] = dict()\n",
        "    self.rewards[state_id] = dict()\n",
        "    if terminal:\n",
        "      self.terminal_states.add(state_id)\n",
        "  \n",
        "  def add_action(self, state_id, action_id, reward, tr_probs, prob_tol=1e-10):\n",
        "    if state_id not in self.states:\n",
        "      raise KeyError(f'state {state_id} does not exist')\n",
        "    if state_id in self.terminal_states:\n",
        "      raise ValueError(f'state {state_id} is a terminal state')\n",
        "    if action_id in self.transition_probs[state_id]:\n",
        "      raise ValueError(f'action {action_id} for state {state_id} already exists')\n",
        "    total_prob = 0.0\n",
        "    for sprime_id, tr_prob in tr_probs.items():\n",
        "      if sprime_id not in self.states:\n",
        "        raise ValueError(f'invalid state {sprime_id} in action {action_id}')\n",
        "      if not 0.0 <= tr_prob <= 1.0:\n",
        "        raise ValueError(f'invalid probability for {sprime_id} in action {action_id}')\n",
        "      total_prob += tr_prob\n",
        "    if np.abs(total_prob - 1.0) > prob_tol:\n",
        "      raise ValueError(f'probabilites don\\'t add to one for state {state_id} in action {action_id}')\n",
        "    self.transition_probs[state_id][action_id] = defaultdict(lambda : 0.0) \n",
        "    for s, p in tr_probs.items():\n",
        "      self.transition_probs[state_id][action_id][s] = p\n",
        "    self.rewards[state_id][action_id] = reward\n",
        "    \n",
        "  def pretty_print(self):\n",
        "    for state_id in self.states:\n",
        "      print(f'State {state_id}:')\n",
        "      if state_id in self.terminal_states:\n",
        "        print('  Terminal state')\n",
        "        continue\n",
        "      for action_id, tr_probs in self.transition_probs[state_id].items():\n",
        "        print(f'  Action {action_id}:')\n",
        "        print(f'    Reward: {self.rewards[state_id][action_id]}')\n",
        "        print(f'    Transition probabilities: ', end='')\n",
        "        for sprime_id, tr_prob in tr_probs.items():\n",
        "          print(f'({sprime_id}, {tr_prob})', end=' ')\n",
        "        print()\n",
        "    \n",
        "  def value_function(self, policy, gamma, iterations=1000):\n",
        "    value = dict((state_id, 0.0) for state_id in self.states)\n",
        "    for _ in range(iterations):\n",
        "      for state_id in self.states:\n",
        "        if state_id in self.terminal_states:\n",
        "          value[state_id] = 0.0\n",
        "          continue\n",
        "        value[state_id] = sum(\n",
        "            action_prob * (self.rewards[state_id][action_id] + \n",
        "                           gamma * \n",
        "                           sum(tr_prob * value[sprime_id] \n",
        "                               for sprime_id, tr_prob \n",
        "                               in self.transition_probs[state_id][action_id].items()))\n",
        "            for action_id, action_prob \n",
        "            in policy[state_id].items())\n",
        "    return value\n",
        "\n",
        "  def policy_iteration(self, gamma, tolerance=1E-5, iterations=1000, value_iterations=20):\n",
        "    policy = dict() # This is a deterministic policy\n",
        "    for state_id in self.states_list:\n",
        "      policy[state_id] = choice(list(self.transition_probs[state_id].keys()))\n",
        "    prev_value = dict((state_id, 0.0) for state_id in self.states_list)\n",
        "    print(prev_value)\n",
        "    for _ in range(iterations):\n",
        "      # Compute value of current policy\n",
        "      value = prev_value.\n",
        "      for _ in range(value_iterations)\n",
        "\n",
        "\n",
        "\n",
        "  def transition_matrix(self, policy):\n",
        "    n = len(self.states_list)\n",
        "    tm = np.zeros((n, n), np.float64)\n",
        "    for i, s in enumerate(self.states_list):\n",
        "      for j, sp in enumerate(self.states_list):\n",
        "        tm[i][j] =sum(policy[s][a] * \n",
        "                      self.transition_probs[s][a][sp] \n",
        "                      for a in policy[s])\n",
        "    return tm\n",
        "\n",
        "  def reward_vector(self, policy):\n",
        "    v = np.zeros(len(self.states_list), np.float64)\n",
        "    for i, s in enumerate(self.states_list):\n",
        "      v[i] = sum(policy[s][a] * self.rewards[s][a] for a in policy[s])\n",
        "    return v"
      ],
      "metadata": {
        "id": "40P5MICVh9RI"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tiny_robot.policy_iteration(0.8)"
      ],
      "metadata": {
        "id": "so-Nw10pbHQa",
        "outputId": "ff01d8bd-76d0-4fda-f2ec-10334264de0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an example, let's implement a MDP corresponding to the tiny robot example."
      ],
      "metadata": {
        "id": "C9o4tXKVj_bS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tiny_robot = MDP()\n",
        "tiny_robot.add_state(1)\n",
        "tiny_robot.add_state(2)\n",
        "tiny_robot.add_state(3)\n",
        "tiny_robot.add_state(4)"
      ],
      "metadata": {
        "id": "DuifXRA-p5Mk"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tiny_robot.add_action(1, 'A', 1, {1: 1/3, 2: 2/3})\n",
        "tiny_robot.add_action(1, 'B', 4, {2: 1/2, 4: 1/2})\n",
        "tiny_robot.add_action(2, 'A', 2, {2: 1/3, 3: 2/3})\n",
        "tiny_robot.add_action(2, 'B', 3, {1: 1/2, 3: 1/2})\n",
        "tiny_robot.add_action(3, 'A', 3, {3: 1/3, 4: 2/3})\n",
        "tiny_robot.add_action(3, 'B', 2, {2: 1/2, 4: 1/2})\n",
        "tiny_robot.add_action(4, 'A', 4, {4: 1/3, 1: 2/3})\n",
        "tiny_robot.add_action(4, 'B', 1, {1: 1/2, 3: 1/2})"
      ],
      "metadata": {
        "id": "UKZq0vb4p5I-"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tiny_robot.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSdmmb-sp5FS",
        "outputId": "a2404d17-4f95-4f57-bea7-05cecdc774ae"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State 1:\n",
            "  Action A:\n",
            "    Reward: 1\n",
            "    Transition probabilities: (1, 0.3333333333333333) (2, 0.6666666666666666) \n",
            "  Action B:\n",
            "    Reward: 4\n",
            "    Transition probabilities: (2, 0.5) (4, 0.5) \n",
            "State 2:\n",
            "  Action A:\n",
            "    Reward: 2\n",
            "    Transition probabilities: (2, 0.3333333333333333) (3, 0.6666666666666666) \n",
            "  Action B:\n",
            "    Reward: 3\n",
            "    Transition probabilities: (1, 0.5) (3, 0.5) \n",
            "State 3:\n",
            "  Action A:\n",
            "    Reward: 3\n",
            "    Transition probabilities: (3, 0.3333333333333333) (4, 0.6666666666666666) \n",
            "  Action B:\n",
            "    Reward: 2\n",
            "    Transition probabilities: (2, 0.5) (4, 0.5) \n",
            "State 4:\n",
            "  Action A:\n",
            "    Reward: 4\n",
            "    Transition probabilities: (4, 0.3333333333333333) (1, 0.6666666666666666) \n",
            "  Action B:\n",
            "    Reward: 1\n",
            "    Transition probabilities: (1, 0.5) (3, 0.5) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "policy = {\n",
        "    1: {'A': 1/2, 'B': 1/2},\n",
        "    2: {'A': 1/4, 'B': 3/4},\n",
        "    3: {'A': 2/3, 'B': 1/3},\n",
        "    4: {'A':   0, 'B':   1}\n",
        "}"
      ],
      "metadata": {
        "id": "I_nxg4ggp5BX"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "value = tiny_robot.value_function(policy, gamma=0.8, iterations=1000)\n",
        "for state_id, v in value.items():\n",
        "    print(f'{state_id}: {v}')"
      ],
      "metadata": {
        "id": "OKzhFJPXp4-E",
        "outputId": "ec47c850-ffdc-4c2d-cf35-4ebbc0396de7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: 11.612474812236673\n",
            "2: 11.87213775416743\n",
            "3: 11.185198754350612\n",
            "4: 10.119069426634914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a check, let's compute the transition matrix corresponding to the given policy:"
      ],
      "metadata": {
        "id": "681JcEdwkKQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tm = tiny_robot.transition_matrix(policy)\n",
        "tm"
      ],
      "metadata": {
        "id": "Ux4AnyZLkJmL",
        "outputId": "b1cc6ff4-7516-4f66-f0ee-f3a6f06a031b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.16666667, 0.58333333, 0.        , 0.25      ],\n",
              "       [0.375     , 0.08333333, 0.54166667, 0.        ],\n",
              "       [0.        , 0.16666667, 0.22222222, 0.61111111],\n",
              "       [0.5       , 0.        , 0.5       , 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = tiny_robot.reward_vector(policy)\n",
        "b"
      ],
      "metadata": {
        "id": "Z2WcvPP4p43C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31bdf0d8-4763-42be-eb9f-997c94d50e8c"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.5       , 2.75      , 2.66666667, 1.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.8\n",
        "M = np.eye(4) - gamma * tm\n",
        "M"
      ],
      "metadata": {
        "id": "VIgtYQxwp4Xk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a95d8ac-9762-4c3e-a1ae-5808cea6c83c"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.86666667, -0.46666667,  0.        , -0.2       ],\n",
              "       [-0.3       ,  0.93333333, -0.43333333,  0.        ],\n",
              "       [ 0.        , -0.13333333,  0.82222222, -0.48888889],\n",
              "       [-0.4       ,  0.        , -0.4       ,  1.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.linalg.solve(M, b)"
      ],
      "metadata": {
        "id": "-Gvot9G3v1NP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ec83043-e4a8-42ce-a1fb-fe4f2b44ae27"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([11.61247481, 11.87213775, 11.18519875, 10.11906943])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tiny_robot.policy_iteration(0.8)"
      ],
      "metadata": {
        "id": "Es52YIYNZ5nL",
        "outputId": "6df824c6-a00c-4751-ec19-d55cef73f8a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-1a31393ac453>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtiny_robot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-46-06b7b5ecc875>\u001b[0m in \u001b[0;36mpolicy_iteration\u001b[0;34m(self, gamma, tolerance, iterations, value_iterations)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstate_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m       \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransition_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m       \u001b[0mrandom_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{state_id} {random_action}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/random.py\u001b[0m in \u001b[0;36mchoice\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot choose from an empty sequence'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'dict_keys' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gw = MDP()\n",
        "n = 4\n",
        "for i in range(n):\n",
        "  for j in range(n):\n",
        "    if (i == j == 0) or (i == j == n-1):\n",
        "      gw.add_state((i, j), terminal=True)\n",
        "    else:\n",
        "      gw.add_state((i, j))\n",
        "# Interior cells\n",
        "for i in range(1, n - 1):\n",
        "  for j in range(1, n - 1):\n",
        "    gw.add_action((i, j), 'U', -1, {(i - 1, j): 1})\n",
        "    gw.add_action((i, j), 'L', -1, {(i, j - 1): 1})\n",
        "    gw.add_action((i, j), 'D', -1, {(i + 1, j): 1})\n",
        "    gw.add_action((i, j), 'R', -1, {(i, j + 1): 1})\n",
        "# Top and bottom borders, not corners\n",
        "for j in range(1, n - 1):\n",
        "  gw.add_action((0, j), 'U', -1, {(0, j): 1})\n",
        "  gw.add_action((0, j), 'L', -1, {(0, j - 1): 1})\n",
        "  gw.add_action((0, j), 'D', -1, {(1, j): 1})\n",
        "  gw.add_action((0, j), 'R', -1, {(0, j + 1): 1})\n",
        "  gw.add_action((n - 1, j), 'U', -1, {(n - 2, j): 1})\n",
        "  gw.add_action((n - 1, j), 'L', -1, {(n - 1, j - 1): 1})\n",
        "  gw.add_action((n - 1, j), 'D', -1, {(n - 1, j): 1})\n",
        "  gw.add_action((n - 1, j), 'R', -1, {(n - 1, j + 1): 1})\n",
        "# Right and left borders, not corners\n",
        "for i in range(1, n - 1):\n",
        "  gw.add_action((i, 0), 'U', -1, {(i - 1, 0): 1})\n",
        "  gw.add_action((i, 0), 'L', -1, {(i, 0): 1})\n",
        "  gw.add_action((i, 0), 'D', -1, {(i + 1, 0): 1})\n",
        "  gw.add_action((i, 0), 'R', -1, {(i, 1): 1})\n",
        "  gw.add_action((i, n - 1), 'U', -1, {(i - 1, n - 1): 1})\n",
        "  gw.add_action((i, n - 1), 'L', -1, {(i, n - 2): 1})\n",
        "  gw.add_action((i, n - 1), 'D', -1, {(i + 1, n - 1): 1})\n",
        "  gw.add_action((i, n - 1), 'R', -1, {(i, n - 1): 1})\n",
        "# Corners\n",
        "# (0, 0) and (n-1, n-1) are ommited because they are terminal states\n",
        "gw.add_action((0, n - 1), 'U', -1, {(0, n - 1): 1})\n",
        "gw.add_action((0, n - 1), 'L', -1, {(0, n - 2): 1})\n",
        "gw.add_action((0, n - 1), 'D', -1, {(1, n - 1): 1})\n",
        "gw.add_action((0, n - 1), 'R', -1, {(0, n - 1): 1})\n",
        "gw.add_action((n - 1, 0), 'U', -1, {(n - 2, 0): 1})\n",
        "gw.add_action((n - 1, 0), 'L', -1, {(n - 1, 0): 1})\n",
        "gw.add_action((n - 1, 0), 'D', -1, {(n - 1, 0): 1})\n",
        "gw.add_action((n - 1, 0), 'R', -1, {(n - 1, 1): 1})"
      ],
      "metadata": {
        "id": "QwEIfzDRkkLx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now print all the information about the MDP. This also demonstrates how to access the elements defining the structure of the chain."
      ],
      "metadata": {
        "id": "kYUpHf5OxxUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gw.pretty_print()"
      ],
      "metadata": {
        "id": "4Rh72g4_iq4z",
        "outputId": "6daad3e4-92cf-42b5-c065-c2457c8db908",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State (0, 0):\n",
            "  Terminal state\n",
            "State (0, 1):\n",
            "  Action U:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((0, 1), 1) \n",
            "  Action L:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((0, 0), 1) \n",
            "  Action D:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((1, 1), 1) \n",
            "  Action R:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((0, 2), 1) \n",
            "State (0, 2):\n",
            "  Action U:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((0, 2), 1) \n",
            "  Action L:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((0, 1), 1) \n",
            "  Action D:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((1, 2), 1) \n",
            "  Action R:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((0, 3), 1) \n",
            "State (0, 3):\n",
            "  Action U:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((0, 3), 1) \n",
            "  Action L:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((0, 2), 1) \n",
            "  Action D:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((1, 3), 1) \n",
            "  Action R:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((0, 3), 1) \n",
            "State (1, 0):\n",
            "  Action U:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((0, 0), 1) \n",
            "  Action L:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((1, 0), 1) \n",
            "  Action D:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((2, 0), 1) \n",
            "  Action R:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((1, 1), 1) \n",
            "State (1, 1):\n",
            "  Action U:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((0, 1), 1) \n",
            "  Action L:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((1, 0), 1) \n",
            "  Action D:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((2, 1), 1) \n",
            "  Action R:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((1, 2), 1) \n",
            "State (1, 2):\n",
            "  Action U:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((0, 2), 1) \n",
            "  Action L:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((1, 1), 1) \n",
            "  Action D:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((2, 2), 1) \n",
            "  Action R:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((1, 3), 1) \n",
            "State (1, 3):\n",
            "  Action U:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((0, 3), 1) \n",
            "  Action L:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((1, 2), 1) \n",
            "  Action D:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((2, 3), 1) \n",
            "  Action R:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((1, 3), 1) \n",
            "State (2, 0):\n",
            "  Action U:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((1, 0), 1) \n",
            "  Action L:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((2, 0), 1) \n",
            "  Action D:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((3, 0), 1) \n",
            "  Action R:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((2, 1), 1) \n",
            "State (2, 1):\n",
            "  Action U:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((1, 1), 1) \n",
            "  Action L:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((2, 0), 1) \n",
            "  Action D:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((3, 1), 1) \n",
            "  Action R:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((2, 2), 1) \n",
            "State (2, 2):\n",
            "  Action U:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((1, 2), 1) \n",
            "  Action L:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((2, 1), 1) \n",
            "  Action D:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((3, 2), 1) \n",
            "  Action R:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((2, 3), 1) \n",
            "State (2, 3):\n",
            "  Action U:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((1, 3), 1) \n",
            "  Action L:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((2, 2), 1) \n",
            "  Action D:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((3, 3), 1) \n",
            "  Action R:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((2, 3), 1) \n",
            "State (3, 0):\n",
            "  Action U:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((2, 0), 1) \n",
            "  Action L:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((3, 0), 1) \n",
            "  Action D:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((3, 0), 1) \n",
            "  Action R:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((3, 1), 1) \n",
            "State (3, 1):\n",
            "  Action U:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((2, 1), 1) \n",
            "  Action L:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((3, 0), 1) \n",
            "  Action D:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((3, 1), 1) \n",
            "  Action R:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((3, 2), 1) \n",
            "State (3, 2):\n",
            "  Action U:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((2, 2), 1) \n",
            "  Action L:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((3, 1), 1) \n",
            "  Action D:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((3, 2), 1) \n",
            "  Action R:\n",
            "    Reward: -1\n",
            "    Transition probabilities: ((3, 3), 1) \n",
            "State (3, 3):\n",
            "  Terminal state\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now see how to specify a policy. A (randomized) policy assigns to each state a probability distribution on the set of actions avaiable at that state. As an example, let's consider the policy that assigns equal probabilities to each of the actions associated to a state. In this case, there are 4 actions possible at each state. So we can use the following code to set up a policy:"
      ],
      "metadata": {
        "id": "ewjlGWwsoUZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "action_probs = {'U': 1/4, 'L': 1/4, 'D':1/4, 'R': 1/4}\n",
        "policy = dict()\n",
        "for i in range(n):\n",
        "  for j in range(n):\n",
        "    if (i == j == 0) or (i == j == n-1):\n",
        "      continue \n",
        "    policy[(i,j)] = action_probs"
      ],
      "metadata": {
        "id": "3cHDpjOBkjy7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "value = gw.value_function(policy, 1, 500)\n",
        "for state_id, v in value.items():\n",
        "  print(f'v({state_id})={v}')"
      ],
      "metadata": {
        "id": "P5hx623akjn3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6028ad7-e303-4f9f-a03a-f3c754e99cec"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "v((0, 0))=0.0\n",
            "v((0, 1))=-13.999999999999988\n",
            "v((0, 2))=-19.99999999999998\n",
            "v((0, 3))=-21.99999999999998\n",
            "v((1, 0))=-13.999999999999986\n",
            "v((1, 1))=-17.999999999999982\n",
            "v((1, 2))=-19.99999999999998\n",
            "v((1, 3))=-19.99999999999998\n",
            "v((2, 0))=-19.99999999999998\n",
            "v((2, 1))=-19.99999999999998\n",
            "v((2, 2))=-17.999999999999982\n",
            "v((2, 3))=-13.999999999999986\n",
            "v((3, 0))=-21.99999999999997\n",
            "v((3, 1))=-19.99999999999998\n",
            "v((3, 2))=-13.999999999999986\n",
            "v((3, 3))=0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(gw.terminal_states)"
      ],
      "metadata": {
        "id": "N57RpXNi_lPJ",
        "outputId": "792d1355-42af-4b9a-fb04-e679b7db26cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{(0, 0), (3, 3)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tm = gw.transition_matrix(policy)\n",
        "tm"
      ],
      "metadata": {
        "id": "NUj2Ceeo_lMN",
        "outputId": "2ac8f4c5-4859-4808-854e-fa0a48384266",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-fe9f3567f189>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransition_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-4f26e10a8676>\u001b[0m in \u001b[0;36mtransition_matrix\u001b[0;34m(self, policy)\u001b[0m\n\u001b[1;32m     77\u001b[0m         tm[i][j] =sum(policy[s][a] * \n\u001b[1;32m     78\u001b[0m                       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransition_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                       for a in policy[s])\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: (0, 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W9rmVPB8_lJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c96F0Dje_lHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uPLFjof4_lDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sikkiui7_lA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VqdzwgBh_k-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "26nzKvvt_k7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DbcsNIlQ_k4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fRDT-s1I_k2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VF4Q-h43_kzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A randomized policy $\\pi$ associates to each state a probability distribution on the set of all actions available at that state. Let's first consider the policy that assumes all actions at each state are equally likely:"
      ],
      "metadata": {
        "id": "nzInxVuTeMkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy = [[None for j in range(n)] for i in range(n)]\n",
        "# Inner cells\n",
        "for i in range(0, n):\n",
        "  for j in range(0,n):\n",
        "    p = 1 / len(states[i][j])\n",
        "    policy[i][j] = dict()\n",
        "    for key in states[i][j]:\n",
        "      policy[i][j][key] = p"
      ],
      "metadata": {
        "id": "OKl_3uRIKRwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now compute the value $V_{\\pi}$ of this policy. We first derive a mathematical equation and then show how to implement its solution in Python. Suppose that the process currently is at state $s$. We then choose an action according to the probability distribution $\\pi(\\cdot|s)$. Let $a$ be the resulting action. Then, we receive a reward $r(s,a)$. So, the expected immediate reward when in state $s$ is:\n",
        "$$\n",
        "\\sum_{a\\in A(s)}\\pi(a|s)r(s,a)\n",
        "$$\n",
        "Then the process transitions to a new state according to the probability distribution $P(\\cdot|s,a)$. The expected future cost is:\n",
        "$$\n",
        "\\sum_{a\\in A(s)}\\pi(a|s)\\sum_{u}P(u|s,a)V_{\\pi}(u)\n",
        "$$\n",
        "We conclude that the system equations for $V_{\\pi}(\\cdot)$ is:\n",
        "$$\n",
        "V_{\\pi}(s) = \\sum_{a \\in A(s)}\\pi(a|s)\\left[r(s,a)+\\sum_{u}P(u|s,a)V_{\\pi}(u)\\right]\n",
        "$$\n",
        "Notice that this is a linear system on the unknown value function $V_{\\pi}(\\cdot)$. Instead of using a standard method (such as Gaussian Elimination), it use use an interactive method to compute the solution. We use the Gauss-Seidel method, which has a particularly simple implementation in this case.\n",
        "\n",
        "We choose an initial approximation for the value function, $V_{\\pi}(s)$, by choosing random values or simply using zeros. Then, we iterate the formula for $V_{\\pi}$ given above. This is implemented in the following code. \n"
      ],
      "metadata": {
        "id": "fijFZu4cgj6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "niter = 100\n",
        "vfunc = [[0.0 for j in range(n)] for i in range(n)]\n",
        "for _ in range(niter):\n",
        "  for i in range(n):\n",
        "    for j in range(n):\n",
        "      for a, p in policy[i][j].items():\n",
        "        acc = states[i][j][a]['reward']\n",
        "        for s, q in states[i][j][a]['tprobs'].items():\n",
        "          k, l = s\n",
        "          acc += q * vfunc[k][l]\n",
        "        vfunc[i][j] = p * acc"
      ],
      "metadata": {
        "id": "tM5EnnFtfSBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(n):\n",
        "  for j in range(n):\n",
        "    print(f\"{vfunc[i][j]:8.5}\", end=' ')\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOYe_FHUp7bc",
        "outputId": "61f03dec-6844-4993-a106-d2893d82934b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     0.0 -0.49417 -0.49417 -0.74126 \n",
            "-0.45688 -0.34266 -0.34266 -0.45688 \n",
            "-0.49417 -0.37063 -0.37063 -0.48252 \n",
            "-0.74126 -0.48252 -0.48252      0.0 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key, item in d.items():\n",
        "  print(key, item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "Kb-OmOLAIWb3",
        "outputId": "7ad69cb3-9222-4435-efa4-d39dabd2a078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-477afb0e742d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code is to check the result"
      ],
      "metadata": {
        "id": "aRCpAu_APbjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 4\n",
        "v = [[0.0 for i in range(n)] for j in range(n)]\n",
        "for _ in range(30):\n",
        "  # Inner cells \n",
        "  for i in range(1, n-1):\n",
        "    for j in range(1, n-1):\n",
        "      v[i][j] = -1 + (1/4) * (v[i-1][j] + v[i][j-1] + v[i+1,j] + v[i][j+1])\n",
        "  # Top and bottom rows\n",
        "    for j in range(1, n-1):\n",
        "      v[j][0] = -1 + (1/3) * (v[])"
      ],
      "metadata": {
        "id": "V12p94E7IZ8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = {'a':1, 'b':2, 'c':4}\n",
        "for elem, key in enumerate(d):\n",
        "  print(elem, key)"
      ],
      "metadata": {
        "id": "I1KVgWzuR52A",
        "outputId": "abf89fa8-d848-4a69-df98-993073fadb63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 a\n",
            "1 b\n",
            "2 c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oZJuxGBTVr99"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}