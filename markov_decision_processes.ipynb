{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM/A9Usoa8oNuJQzZKfKXwu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lfmartins/markov-decision-processes/blob/main/markov_decision_processes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an example of MDP, let's consider a robot that moves in a $4\\times4$ grid. The objective is to mimimize the number of steps the robot takes to reach the one of the target cells $(0,0)$ and $(3,3)$. \n",
        "\n",
        "If the robot is in grid cell $(i,j)$, there are at most 4 actions, corresponding to the robot moving north, east, south and west (if the robot is on one of the edges not all actions are allowed). The reward of taking any one of the actions is $-1$, except at the target cells, in which case the reward is zero. \n",
        "\n",
        "A discrete MDP is defined in terms of two functions:\n",
        "\n",
        "- $p(s' \\mid s, a)$ is the probability that the process will be in state $s'$ at time $t$, given that the process is in sate $s$ at time $t-1$ and action $a$ is selected. (The notation $p(s' \\mid s,a)$ is used to resemble the notation for conditional probability, but it just represents a function of three variables, $s'$, $s$ and $a$.\n",
        "- $r(s,a)$ is the reward obtained for selecting action $a$.\n",
        "\n",
        "A *randomized policy* is represented by function of two variables $\\pi(a|s)$ that, to every action $a$ available in state $s$, associates the probability that action $a$ is chosen when in state $s$.\n",
        "\n",
        "To represent a MDP in Python we define the following data structures.\n",
        "\n",
        "A MDP is represented by objects of the class `MDP`. This is a very thin implementation. A `MDP` holds information about a set of states. States are created by the method `add_state()`. When a state is added, the only information that is recorded is the `state_id`. This means that just adding the states does not define the structure of the chain.\n",
        "\n",
        "To each state, we associate a set of actions through the `add_action()` method. An action specifies a reward and a set of transition probabilities."
      ],
      "metadata": {
        "id": "ekGXJ8ufNLFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class MDP(object):\n",
        "  def __init__(self):\n",
        "    self.states = dict()\n",
        "    self.states_list = []\n",
        "    self.terminal_states = set()\n",
        "  \n",
        "  def add_state(self, state_id, terminal=False):\n",
        "    if state_id in self.states:\n",
        "      raise ValueError(f'state {state_id} already exists')\n",
        "    self.states[state_id] = dict()\n",
        "    self.states_list.append(state_id)\n",
        "    if terminal:\n",
        "      self.terminal_states.add(terminal)\n",
        "  \n",
        "  def add_action(self, state_id, action_id, reward, tprobs):\n",
        "    if not state_id in self.states:\n",
        "      raise KeyError(f'state {state_id} does not exist')\n",
        "    if action_id in self.states[state_id]:\n",
        "      raise ValueError('repeated action for state {state_id}')\n",
        "    self.states[state_id][action_id] = {'reward': reward, 'tprobs': tprobs}\n",
        "\n",
        "  def reward(self, state_id, action_id):\n",
        "    return self.states[state_id][action_id]['reward']\n",
        "    \n",
        "  def transition_prob(self, state1_id, action_id, state2_id):\n",
        "    return self.states[state1_id][action_id]['tprobs'][state2_id]\n",
        "\n",
        "  def pretty_print(self):\n",
        "    for state_id in self.states:\n",
        "      print(f'State {state_id}')\n",
        "      for action_id in self.states[state_id]:\n",
        "        print(f'  Action {action_id}:')\n",
        "        print(f'    Reward: {self.reward(state_id, action_id)}')\n",
        "        print(f'    Transition probabilities: ', end='')\n",
        "        for state2_id in self.states[state_id][action_id]['tprobs']:\n",
        "          print(f'({state2_id}, {self.transition_prob(state_id, action_id, state2_id)})', end=' ')\n",
        "        print()\n",
        "    \n",
        "  def value_function(self, policy, gamma, iterations=1000):\n",
        "    value = dict((state_id, 0.0) for state_id in self.states)\n",
        "    for _ in range(iterations):\n",
        "      for state_id in self.states:\n",
        "        if state_id in self.terminal_states:\n",
        "          value[state_id] = 0.0\n",
        "          continue\n",
        "        value[state_id] = sum(\n",
        "          policy[state_id][action_id] * (\n",
        "            self.reward(state_id, action_id) + gamma * sum(\n",
        "              self.transition_prob(state_id, action_id, state1_id) * value[state1_id]\n",
        "              for state1_id in self.states[state_id][action_id]['tprobs']))\n",
        "            for action_id in policy[state_id])\n",
        "      return value\n",
        "\n",
        "  def transition_matrix(self, policy):\n",
        "    n = len(self.states_list)\n",
        "    tm = np.zeros(n, n, np.float64)\n",
        "    for i, s in enumerate(states):\n"
      ],
      "metadata": {
        "id": "40P5MICVh9RI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "f1e82a5b-8169-49f4-c4e0-6ddce31e0e25"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-eabf40d888a7>\"\u001b[0;36m, line \u001b[0;32m58\u001b[0m\n\u001b[0;31m    for i, s in states.enumerate():\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an example, let's implement a MDP corresponding to the tiny robot example."
      ],
      "metadata": {
        "id": "C9o4tXKVj_bS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tiny_robot = MDP()\n",
        "tiny_robot.add_state(1)\n",
        "tiny_robot.add_state(2)\n",
        "tiny_robot.add_state(3)\n",
        "tiny_robot.add_state(4)"
      ],
      "metadata": {
        "id": "DuifXRA-p5Mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tiny_robot.add_action(1, 'A', 1, {1: 1/3, 2: 2/3})\n",
        "tiny_robot.add_action(1, 'B', 4, {2: 1/2, 4: 1/2})\n",
        "tiny_robot.add_action(2, 'A', 2, {2: 1/3, 3: 2/3})\n",
        "tiny_robot.add_action(2, 'B', 3, {1: 1/2, 3: 1/2})\n",
        "tiny_robot.add_action(3, 'A', 3, {3: 1/3, 4: 2/3})\n",
        "tiny_robot.add_action(3, 'B', 2, {2: 1/2, 4: 1/2})\n",
        "tiny_robot.add_action(4, 'A', 4, {4: 1/3, 1: 2/3})\n",
        "tiny_robot.add_action(4, 'B', 1, {1: 1/2, 3: 1/2})"
      ],
      "metadata": {
        "id": "UKZq0vb4p5I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tiny_robot.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSdmmb-sp5FS",
        "outputId": "3a4ccb6f-ad7c-41f9-86f8-12f0c2e25285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State 1\n",
            "  Action A:\n",
            "    Reward: 1\n",
            "    Transition probabilities: (1, 0.3333333333333333) (2, 0.6666666666666666) \n",
            "  Action B:\n",
            "    Reward: 4\n",
            "    Transition probabilities: (2, 0.5) (4, 0.5) \n",
            "State 2\n",
            "  Action A:\n",
            "    Reward: 2\n",
            "    Transition probabilities: (2, 0.3333333333333333) (3, 0.6666666666666666) \n",
            "  Action B:\n",
            "    Reward: 3\n",
            "    Transition probabilities: (1, 0.5) (3, 0.5) \n",
            "State 3\n",
            "  Action A:\n",
            "    Reward: 3\n",
            "    Transition probabilities: (3, 0.3333333333333333) (4, 0.6666666666666666) \n",
            "  Action B:\n",
            "    Reward: 2\n",
            "    Transition probabilities: (2, 0.5) (4, 0.5) \n",
            "State 4\n",
            "  Action A:\n",
            "    Reward: 4\n",
            "    Transition probabilities: (4, 0.3333333333333333) (1, 0.6666666666666666) \n",
            "  Action B:\n",
            "    Reward: 1\n",
            "    Transition probabilities: (1, 0.5) (3, 0.5) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "policy = {\n",
        "    1: {'A': 1/2, 'B': 1/2},\n",
        "    2: {'A': 1/4, 'B': 3/4},\n",
        "    3: {'A': 2/3, 'B': 1/3},\n",
        "    4: {'A':   0, 'B':   1}\n",
        "}"
      ],
      "metadata": {
        "id": "I_nxg4ggp5BX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "value = tiny_robot.value_function(policy, gamma=0.8, iterations=1000)"
      ],
      "metadata": {
        "id": "OKzhFJPXp4-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for state_id, v in value.items():\n",
        "    print(f'{state_id}: {v}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTdsUzABp46p",
        "outputId": "d1eea177-0976-48f2-e3e4-466b41c0289d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: 2.5\n",
            "2: 3.5\n",
            "3: 3.1333333333333333\n",
            "4: 3.2533333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "Z2WcvPP4p43C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "M = np.array([[1,2,3],[2,-1,4],[-3,3,2]])\n",
        "M"
      ],
      "metadata": {
        "id": "H5_HLzaBp4zX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98d53963-822c-47f6-e58f-88658237e869"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  2,  3],\n",
              "       [ 2, -1,  4],\n",
              "       [-3,  3,  2]])"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = np.array([1,3,2]).reshape(3,1)\n",
        "b"
      ],
      "metadata": {
        "id": "8yh9RE7Jp4wR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cab046b6-52a9-4bb7-91d7-1661ca4e6c62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1],\n",
              "       [3],\n",
              "       [2]])"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v = np.linalg.inv(M) @ b\n",
        "v"
      ],
      "metadata": {
        "id": "Wk-qOV_sp4tL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afcdbbfa-59e4-449d-e61a-88c8f2d210b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.62162162],\n",
              "       [-0.56756757],\n",
              "       [ 0.91891892]])"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "M @ v"
      ],
      "metadata": {
        "id": "_sXWqhFYp4lX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "953dcd44-15fb-4ade-fc9b-82be12ee8dc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.],\n",
              "       [3.],\n",
              "       [2.]])"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VIgtYQxwp4Xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gw = MDP()\n",
        "n = 4\n",
        "for i in range(n):\n",
        "  for j in range(n):\n",
        "    gw.add_state((i,j))"
      ],
      "metadata": {
        "id": "l61Zh7Zvj-gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interior cells\n",
        "for i in range(1, n - 1):\n",
        "  for j in range(1, n - 1):\n",
        "    gw.add_action((i, j), 'U', -1, {(i - 1, j): 1})\n",
        "    gw.add_action((i, j), 'L', -1, {(i, j - 1): 1})\n",
        "    gw.add_action((i, j), 'D', -1, {(i + 1, j): 1})\n",
        "    gw.add_action((i, j), 'R', -1, {(i, j + 1): 1})\n",
        "# Top and bottom borders, not corners\n",
        "for j in range(1, n - 1):\n",
        "  gw.add_action((0, j), 'U', -1, {(0, j): 1})\n",
        "  gw.add_action((0, j), 'L', -1, {(0, j - 1): 1})\n",
        "  gw.add_action((0, j), 'D', -1, {(1, j): 1})\n",
        "  gw.add_action((0, j), 'R', -1, {(0, j + 1): 1})\n",
        "  gw.add_action((n - 1, j), 'U', -1, {(n - 1, j): 1})\n",
        "  gw.add_action((n - 1, j), 'L', -1, {(n - 1, j - 1): 1})\n",
        "  gw.add_action((n - 1, j), 'D', -1, {(n - 1, j): 1})\n",
        "  gw.add_action((n - 1, j), 'R', -1, {(n - 1, j + 1): 1})\n",
        "# Right and left borders, not corners\n",
        "for i in range(1, n - 1):\n",
        "  gw.add_action((i, 0), 'U', -1, {(i - 1, 0): 1})\n",
        "  gw.add_action((i, 0), 'L', -1, {(i, 0): 1})\n",
        "  gw.add_action((i, 0), 'D', -1, {(i + 1, 0): 1})\n",
        "  gw.add_action((i, 0), 'R', -1, {(i, 1): 1})\n",
        "  gw.add_action((i, n - 1), 'U', -1, {(i - 1, n - 1): 1})\n",
        "  gw.add_action((i, n - 1), 'L', -1, {(i, n - 2): 1})\n",
        "  gw.add_action((i, n - 1), 'D', -1, {(i, n - 1): 1})\n",
        "  gw.add_action((i, n - 1), 'R', -1, {(i, n - 1): 1})\n",
        "# Corners\n",
        "gw.add_action((0, 0), 'U', 0, {(0, 0): 1})\n",
        "gw.add_action((0, 0), 'L', 0, {(0, 0): 1})\n",
        "gw.add_action((0, 0), 'D', 0, {(0, 0): 1})\n",
        "gw.add_action((0, 0), 'R', 0, {(0, 1): 1})\n",
        "gw.add_action((n - 1, n - 1), 'U', 0, {(n - 1, n - 1): 1})\n",
        "gw.add_action((n - 1, n - 1), 'L', 0, {(n - 1, n - 1): 1})\n",
        "gw.add_action((n - 1, n - 1), 'D', 0, {(n - 1, n - 1): 1})\n",
        "gw.add_action((n - 1, n - 1), 'R', 0, {(n - 1, n - 1): 1})\n",
        "gw.add_action((0, n - 1), 'U', -1, {(0, n - 1): 1})\n",
        "gw.add_action((0, n - 1), 'L', -1, {(0, n - 2): 1})\n",
        "gw.add_action((0, n - 1), 'D', -1, {(1, n - 1): 1})\n",
        "gw.add_action((0, n - 1), 'R', -1, {(0, n - 1): 1})\n",
        "gw.add_action((n - 1, 0), 'U', -1, {(n - 2, 0): 1})\n",
        "gw.add_action((n - 1, 0), 'L', -1, {(n - 1, 0): 1})\n",
        "gw.add_action((n - 1, 0), 'D', -1, {(n - 1, 0): 1})\n",
        "gw.add_action((n - 1, 0), 'R', -1, {(n - 1, 1): 1})"
      ],
      "metadata": {
        "id": "QwEIfzDRkkLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now print all the information about the MDP. This also demonstrates how to access the elements defining the structure of the chain."
      ],
      "metadata": {
        "id": "kYUpHf5OxxUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for state_id, actions in gw.states.items():\n",
        "  print(f'State {state_id}:')\n",
        "  state_info = gw.states[state_id]\n",
        "  for action_id, action_info in state_info.items():\n",
        "    print(f'  Action {action_id}: Reward: {action_info[\"reward\"]}')\n",
        "    for next_state_id, tprob in action_info['tprobs'].items():\n",
        "      print(f'    {next_state_id}: {tprob}')"
      ],
      "metadata": {
        "id": "wFWiAoxTkj_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now see how to specify a policy. A (randomized) policy assigns to each state a probability distribution on the set of actions avaiable at that state. As an example, let's consider the policy that assigns equal probabilities to each of the actions associated to a state. In this case, there are 4 actions possible at each state. So we can use the following code to set up a policy:"
      ],
      "metadata": {
        "id": "ewjlGWwsoUZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "action_probs = {'U': 1/4, 'L': 1/4, 'D':1/4, 'R': 1/4}\n",
        "policy = dict()\n",
        "for i in range(n):\n",
        "  for j in range(n):\n",
        "    policy[(i,j)] = action_probs"
      ],
      "metadata": {
        "id": "3cHDpjOBkjy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "value = gw.value_function(policy, 1, {(0,0), (n-1, n-1)}, 5000)\n",
        "for state_id, v in value.items():\n",
        "  print(f'value)"
      ],
      "metadata": {
        "id": "P5hx623akjn3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2509a12c-d641-4e86-bac5-c68547c95b24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{(0, 0): 0.0, (0, 1): -8.231134329204314, (0, 2): -11.614077205594233, (0, 3): -11.631948674342972, (1, 0): -4.792491442094327, (1, 1): -9.079325782018712, (1, 2): -10.979148613235413, (1, 3): -10.176209877960307, (2, 0): -5.840639986358598, (2, 1): -8.314528743540793, (2, 2): -9.046981587368405, (2, 3): -8.852809166144027, (3, 0): -4.80741689927569, (3, 1): -5.291167618417454, (3, 2): -4.041439826553388, (3, 3): 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the following MDP as a test case. The state space is $\\mathscr{S}=\\{1,2,3,4\\}$. In each state, there are two possible actions, labeled $A$ and $B$. The following tables specify the transition probabilities and rewards for each action:\n",
        "\n"
      ],
      "metadata": {
        "id": "ZQdsbm0W_mqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_mdp = MDP()\n",
        "example_mdp.add_state(1)\n",
        "example_mdp.add_state(2)\n",
        "example_mdp.add_state(3)\n",
        "example_mdp.add_state(4)"
      ],
      "metadata": {
        "id": "LgPBHIJ6kjdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_mdp.add_action(1, 'A', 1, {1: 1/3, 2})"
      ],
      "metadata": {
        "id": "qTNDjPhPkjQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N57RpXNi_lPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NUj2Ceeo_lMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W9rmVPB8_lJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c96F0Dje_lHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uPLFjof4_lDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sikkiui7_lA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VqdzwgBh_k-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "26nzKvvt_k7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DbcsNIlQ_k4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fRDT-s1I_k2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VF4Q-h43_kzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  def value_function(self, policy, discount_factor  , iterations=1000):\n",
        "    value = dict(zip(self.states, len(self.states) * [0.0]))\n",
        "    for n in range(iterations):\n",
        "      for state_id, state_info in self.states.items():\n",
        "        if state_id in absorbing_states:\n",
        "          value[state_id] = 0.0\n",
        "          continue\n",
        "        acc = 0\n",
        "        for action_id, tprob in policy[state_id].items():\n",
        "          acc += (state_info[action_id]['reward'] +\n",
        "                 discount_factor * \n",
        "                  sum(prob * value[next_state_id] \n",
        "                      for next_state_id, prob\n",
        "                      in state_info[action_id]['tprobs'].items()))\n",
        "          value[state_id] = tprob * acc\n",
        "    return value "
      ],
      "metadata": {
        "id": "psYKvjSV_kw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 4  # Grid size\n",
        "# We initialize the grid to \"empty\" cells\n",
        "states = [[None for j in range(n)] for i in range(n)]\n",
        "states"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xcNvVVkMdGl",
        "outputId": "6bfff656-fac9-41b1-8688-139b0f40be8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[None, None, None, None],\n",
              " [None, None, None, None],\n",
              " [None, None, None, None],\n",
              " [None, None, None, None]]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inner cells\n",
        "for i in range(1, n-1):\n",
        "  for j in range(1, n-1):\n",
        "    states[i][j] = {\n",
        "        'N': {'reward': -1, 'tprobs': {(i-1, j): 1, (i-1, j): 0, (i, j+1): 0, (i+1, j): 0}},\n",
        "        'W': {'reward': -1, 'tprobs': {(i-1, j): 0, (i-1, j): 1, (i, j+1): 0, (i+1, j): 0}},\n",
        "        'S': {'reward': -1, 'tprobs': {(i-1, j): 0, (i-1, j): 0, (i, j+1): 1, (i+1, j): 0}},\n",
        "        'E': {'reward': -1, 'tprobs': {(i-1, j): 0, (i-1, j): 0, (i, j+1): 0, (i+1, j): 1}},\n",
        "    }\n",
        "# Edge cells, not corners\n",
        "for j in range(1, n-1):\n",
        "  # North edge\n",
        "  states[0][j] = {\n",
        "      'W': {'reward': -1, 'tprobs': {(i-1, j): 0, (i-1, j): 1, (i, j+1): 0, (i+1, j): 0}},\n",
        "      'S': {'reward': -1, 'tprobs': {(i-1, j): 0, (i-1, j): 0, (i, j+1): 1, (i+1, j): 0}},\n",
        "      'E': {'reward': -1, 'tprobs': {(i-1, j): 0, (i-1, j): 0, (i, j+1): 0, (i+1, j): 1}},\n",
        "  }\n",
        "  # West edge\n",
        "  states[j][0] = {\n",
        "      'N': {'reward': -1, 'tprobs': {(i-1, j): 1, (i-1, j): 0, (i, j+1): 0, (i+1, j): 0}},\n",
        "      'W': {'reward': -1, 'tprobs': {(i-1, j): 0, (i-1, j): 1, (i, j+1): 0, (i+1, j): 0}},\n",
        "      'S': {'reward': -1, 'tprobs': {(i-1, j): 0, (i-1, j): 0, (i, j+1): 1, (i+1, j): 0}},\n",
        "  }\n",
        "  # South edge \n",
        "  states[n-1][j] = {\n",
        "      'N': {'reward': -1, 'tprobs': {(i-1, j): 1, (i-1, j): 0, (i, j+1): 0, (i+1, j): 0}},\n",
        "      'W': {'reward': -1, 'tprobs': {(i-1, j): 0, (i-1, j): 1, (i, j+1): 0, (i+1, j): 0}},\n",
        "      'E': {'reward': -1, 'tprobs': {(i-1, j): 0, (i-1, j): 0, (i, j+1): 0, (i+1, j): 1}},\n",
        "  }\n",
        "  # East edge\n",
        "  states[j][n-1] = {\n",
        "      'N': {'reward': -1, 'tprobs': {(i-1, j): 1, (i-1, j): 0, (i, j+1): 0, (i+1, j): 0}},\n",
        "      'W': {'reward': -1, 'tprobs': {(i-1, j): 0, (i-1, j): 1, (i, j+1): 0, (i+1, j): 0}},\n",
        "      'S': {'reward': -1, 'tprobs': {(i-1, j): 0, (i-1, j): 0, (i, j+1): 1, (i+1, j): 0}},\n",
        "  }\n",
        "# Nortwest corner (terminal state)\n",
        "states[0][0] = {\n",
        "    'T': {'reward': 0, 'tprobs': {(0,0): 1}}\n",
        "}\n",
        "# Southwest corner\n",
        "states[n-1][0] = {\n",
        "    'N': {'reward': -1, 'tprobs': {(i-1, j): 1, (i-1, j): 0, (i, j+1): 0, (i+1, j): 0}},\n",
        "    'E': {'reward': -1, 'tprobs': {(i-1, j): 0, (i-1, j): 0, (i, j+1): 0, (i+1, j): 1}},\n",
        "}\n",
        "# Southeast corner (terminal state)\n",
        "states[n-1][n-1] = {\n",
        "    'T': {'reward': 0, 'tprobs': {(0,0): 1}}\n",
        "}\n",
        "# Northeast corner \n",
        "states[0][n-1] = {\n",
        "    'W': {'reward': -1, 'tprobs': {(i-1, j): 0, (i-1, j): 1, (i, j+1): 0, (i+1, j):0}},\n",
        "    'S': {'reward': -1, 'tprobs': {(i-1, j): 0, (i-1, j): 0, (i, j+1): 1, (i+1, j):0}},\n",
        "}"
      ],
      "metadata": {
        "id": "xhPFuu6ZAaAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A randomized policy $\\pi$ associates to each state a probability distribution on the set of all actions available at that state. Let's first consider the policy that assumes all actions at each state are equally likely:"
      ],
      "metadata": {
        "id": "nzInxVuTeMkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy = [[None for j in range(n)] for i in range(n)]\n",
        "# Inner cells\n",
        "for i in range(0, n):\n",
        "  for j in range(0,n):\n",
        "    p = 1 / len(states[i][j])\n",
        "    policy[i][j] = dict()\n",
        "    for key in states[i][j]:\n",
        "      policy[i][j][key] = p"
      ],
      "metadata": {
        "id": "OKl_3uRIKRwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now compute the value $V_{\\pi}$ of this policy. We first derive a mathematical equation and then show how to implement its solution in Python. Suppose that the process currently is at state $s$. We then choose an action according to the probability distribution $\\pi(\\cdot|s)$. Let $a$ be the resulting action. Then, we receive a reward $r(s,a)$. So, the expected immediate reward when in state $s$ is:\n",
        "$$\n",
        "\\sum_{a\\in A(s)}\\pi(a|s)r(s,a)\n",
        "$$\n",
        "Then the process transitions to a new state according to the probability distribution $P(\\cdot|s,a)$. The expected future cost is:\n",
        "$$\n",
        "\\sum_{a\\in A(s)}\\pi(a|s)\\sum_{u}P(u|s,a)V_{\\pi}(u)\n",
        "$$\n",
        "We conclude that the system equations for $V_{\\pi}(\\cdot)$ is:\n",
        "$$\n",
        "V_{\\pi}(s) = \\sum_{a \\in A(s)}\\pi(a|s)\\left[r(s,a)+\\sum_{u}P(u|s,a)V_{\\pi}(u)\\right]\n",
        "$$\n",
        "Notice that this is a linear system on the unknown value function $V_{\\pi}(\\cdot)$. Instead of using a standard method (such as Gaussian Elimination), it use use an interactive method to compute the solution. We use the Gauss-Seidel method, which has a particularly simple implementation in this case.\n",
        "\n",
        "We choose an initial approximation for the value function, $V_{\\pi}(s)$, by choosing random values or simply using zeros. Then, we iterate the formula for $V_{\\pi}$ given above. This is implemented in the following code. \n"
      ],
      "metadata": {
        "id": "fijFZu4cgj6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "niter = 100\n",
        "vfunc = [[0.0 for j in range(n)] for i in range(n)]\n",
        "for _ in range(niter):\n",
        "  for i in range(n):\n",
        "    for j in range(n):\n",
        "      for a, p in policy[i][j].items():\n",
        "        acc = states[i][j][a]['reward']\n",
        "        for s, q in states[i][j][a]['tprobs'].items():\n",
        "          k, l = s\n",
        "          acc += q * vfunc[k][l]\n",
        "        vfunc[i][j] = p * acc"
      ],
      "metadata": {
        "id": "tM5EnnFtfSBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(n):\n",
        "  for j in range(n):\n",
        "    print(f\"{vfunc[i][j]:8.5}\", end=' ')\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOYe_FHUp7bc",
        "outputId": "61f03dec-6844-4993-a106-d2893d82934b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     0.0 -0.49417 -0.49417 -0.74126 \n",
            "-0.45688 -0.34266 -0.34266 -0.45688 \n",
            "-0.49417 -0.37063 -0.37063 -0.48252 \n",
            "-0.74126 -0.48252 -0.48252      0.0 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key, item in d.items():\n",
        "  print(key, item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "Kb-OmOLAIWb3",
        "outputId": "7ad69cb3-9222-4435-efa4-d39dabd2a078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-477afb0e742d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code is to check the result"
      ],
      "metadata": {
        "id": "aRCpAu_APbjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 4\n",
        "v = [[0.0 for i in range(n)] for j in range(n)]\n",
        "for _ in range(30):\n",
        "  # Inner cells \n",
        "  for i in range(1, n-1):\n",
        "    for j in range(1, n-1):\n",
        "      v[i][j] = -1 + (1/4) * (v[i-1][j] + v[i][j-1] + v[i+1,j] + v[i][j+1])\n",
        "  # Top and bottom rows\n",
        "    for j in range(1, n-1):\n",
        "      v[j][0] = -1 + (1/3) * (v[])"
      ],
      "metadata": {
        "id": "V12p94E7IZ8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I1KVgWzuR52A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}