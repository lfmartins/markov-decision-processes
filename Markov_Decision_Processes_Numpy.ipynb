{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMB1zQwlxCq6ih4Qc0EVfcm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lfmartins/markov-decision-processes/blob/main/Markov_Decision_Processes_Numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminaries\n",
        "\n",
        "## Optional installation of Python 3.11\n",
        "Run the following cell to install and use Python 3.11 in this notebook. As of the time of this writing, the default version of Python in colab is 3.8.\n",
        "\n",
        "Output is supressed, and installation may take a couple minutes to finish. \n",
        "\n",
        "Remove `%%capture` to see the shell messages."
      ],
      "metadata": {
        "id": "Pjk_AI-9ZQze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!sudo apt-get update -y\n",
        "!sudo apt-get install python3.11\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 2"
      ],
      "metadata": {
        "id": "2ZL3ivUevlBu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if installation is successful:"
      ],
      "metadata": {
        "id": "V9644YF4ZzXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1eeSZ7bZvcz",
        "outputId": "d55cda60-8d9c-4ccb-fbee-ab782d22d2b0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.8.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n",
        "\n",
        "Run the following cell to import required modules and functions."
      ],
      "metadata": {
        "id": "rzeECdudZ-Lg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZSwezsRuKRdP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import numpy.linalg"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "We want to build a framework for MDPs. The basic ingredients are:\n",
        "\n",
        "- A set of *states* $\\mathscr{S}$. We assume that $\\mathscr{S}$ is finite and let $N=|\\mathscr{S}|$\n",
        "- A set of *actions* $\\mathscr{A}$. We assume that $\\mathscr{A}$ is finite.\n",
        "- For each $s\\in\\mathscr{S}$, a set of *admissible actions for state $s$*, $\\mathscr{A}_s\\subset\\mathscr{A}$.\n",
        "- For each $s,s'\\in\\mathscr{S}$ and $a\\in\\mathscr{A}_s$, a number $p(s'\\,|\\,s,a)\\in[0,1]$.\n",
        "- For each $s\\in\\mathscr{S}$ and $a\\in\\mathscr{A}$, a number $r(s,a)$. We let the set of possible reward be $\\mathscr{R}$. We also assume this set to be finite.\n",
        "\n",
        "We interpret $p(s'\\,|\\,s,a)$ as the probability that, if the agent is in state $s$ and action $a$ is chosen, the agent will next transition to state $s'$. We require that, for each $s$, $a$ we have:\n",
        "$$\n",
        "\\sum_{s'\\in\\mathscr{S}}p(s'\\,|\\,s,a)=1\n",
        "$$\n",
        "\n",
        "The function $r(s,a)$ represents a *reward* received by the agent for visiting state $s$ and choosing action $a$. We can easily extend the definition to cover for randomized rewards.\n",
        "\n",
        "We will only consider infinite-horizon problems, so we only have to deal with stationary policies. A *randomized policy* is a specification, for every $s\\in\\mathscr{S}$ and $a\\in\\mathscr{A}$ of a number $\\pi(a\\,|\\,s)$, interpreted as the probability that action $a$ is chosen when in state $s$. We require:\n",
        "$$\n",
        "0\\le\\pi(a\\,|\\,s)\\le 1,\\quad \\sum_{a\\in\\mathscr{A_s}}\\pi(a\\,|\\,s)=1\n",
        "$$\n",
        "If, for all $s\\in\\mathscr{S}$ there is an $a=a(s)\\in\\mathscr{A}_s$ such that $\\pi(a\\,|\\,s)=1$, we say that $\\pi$ is *deterministic*.\n",
        "\n",
        "The *transition probability matrix* associated to a policy $\\pi$ is a $N\\times N$ matrix $Q$ defined by:\n",
        "$$\n",
        "Q_{ss'}=\\sum_{a\\in\\mathscr{A}_s}\\pi(a\\,|\\,s)p(s'\\,|\\,s,a)\n",
        "$$\n",
        "It is easy to see that this is a stochastic matrix. Thus, if $\\pi_0$ is a probability distribution on $\\mathscr{S}$, there is a Markov chain $\\{(S_t,A_t,R_t)\\}_{t\\ge 0}\\subset \\mathscr{S}\\times\\mathscr{A}\\times\\mathscr{R}$ such that:\n",
        "\n",
        "- $\\mathbb{P}_{\\pi,\\pi_0}\\left[S_t=s'\\,|\\,S_{t-1}=s, A_{t-1}=a\\right]=p(s'\\,|\\,s,a)$\n",
        "- $\\mathbb{P}_{\\pi,\\pi_0}\\left[A_t=a\\,|\\,S_t=s\\right]=\\pi(a\\,|\\,s)$ if $a\\in\\mathscr{A}_t$.\n",
        "- $R_t=r(S_t,A_t)$\n",
        "\n",
        "In general, the initial distribution will not be relevant in the formulas below, so we will write simply $P_\\pi$ to the probability measure corresponding to the Markov chain. We denote the corresponding expected value by $E_\\pi$.\n",
        "\n",
        "Let $0< \\gamma < 1$ we define the *value function* of policy $\\pi$ as:\n",
        "$$\n",
        "V_\\pi(s)=E_\\pi\\left[\\sum_{t=0}^\\infty\\gamma^tR_t\\,|\\,S_0=s\\right]\n",
        "$$\n",
        "\n",
        "This value function is a solution to *Bellman's Equation*:\n",
        "$$\n",
        "V_\\pi(s)=\\sum_{a\\in\\mathscr{A}_s}\\pi(a\\,|\\,s)\\left[r(a,s)+\\sum_{s'\\in\\mathscr{S}}\\gamma p(s'\\,|\\,s,a)V_\\pi(s)\\right]\n",
        "$$\n",
        "The sum is finite since $\\gamma$ is in the interval $(0,1)$.\n",
        "\n",
        "The *optimal value function* is defined as:\n",
        "$$\n",
        "V(s)=\\max\\left\\{V_\\pi(s)\\,|\\,\\text{$\\pi$ is a randomized policy}\\right\\}\n",
        "$$\n",
        "\n",
        "Bellman's Equation for the optimal value function is:\n",
        "$$\n",
        "V^*(s)=\\max_{a\\in\\mathscr{A}_s}\\left\\{r(a,s)+\\sum_{s'\\in\\mathscr{S}}\\gamma p(s'\\,|\\,s,a)V^*(s)\\right\\}\n",
        "$$\n",
        "The optimal policy is always deterministic, and is found by:\n",
        "$$\n",
        "\\pi^*(s)=\\underset{a\\in\\mathscr{A}_s}{\\arg\\max}\\left\\{r(a,s)+\\sum_{s'\\in\\mathscr{S}}\\gamma^tp(s'\\,|\\,s,a)V^*(s)\\right\\}\n",
        "$$\n",
        "Obviously, we have:\n",
        "$$\n",
        "V^*(s)=V_{\\pi^*}(s)\n",
        "$$\n",
        "for all $s\\in\\mathscr{S}$."
      ],
      "metadata": {
        "id": "ESbj5F36KazZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Structures\n",
        "\n",
        "We need to store three things:\n",
        "\n",
        "- The transition probabilities $p(s',s,a)$\n",
        "- The rewards $r(s,a)$\n",
        "- The policy\n",
        "\n",
        "A Markov Decision Process is represented by an object in the class `MDP`. This class records all states and actions. States and actions have unique IDs that are mapped to an integer (according to the order by which they are created). The state and action IDs can be any hashable object.\n",
        "\n",
        "State IDs are stored in a list `__states`, and action IDs are stored in a list `__actions`. States and actions are added to these lists in the order they are created.\n",
        "\n",
        "We call the *index* os a state `s` the position where `s` is stored in the list `__states`. Similarly the index of an action `a` is its position in the list `__actions`\n",
        "\n",
        "For storing the transition probabilities, we use a list `__tp`. Each entry in `__tp` is a dictionary. For each action, this dictionary points to an array containing the transition probabilities.\n",
        "\n",
        "Notice that there is some waste in this description, and in the future we may move to a more sparse representation.\n"
      ],
      "metadata": {
        "id": "XsR5ry1ditiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MDP(object):\n",
        "  def __init__(self, states, actions, mdp_data):\n",
        "    self.__states = states\n",
        "    self.__nstates = len(self.__states)\n",
        "    self.__state_index = {s: i for i, s in enumerate(self.__states)}\n",
        "    self.__actions = actions\n",
        "    self.__nactions = len(self.__actions)\n",
        "    self.__action_index = {a: i for i, a in enumerate(self.__actions)}\n",
        "    \n",
        "    self.__allowed_actions = [set() for _ in range(self.__nstates)]\n",
        "    self.__r = np.zeros((self.__nactions, self.__nstates), dtype=np.float64)\n",
        "    self.__p = np.zeros((self.__nstates, self.__nactions, self.__nstates), dtype=np.float64)\n",
        "    for (s, a), (r, tp) in mdp_data.items():\n",
        "      try:\n",
        "         s_index = self.__state_index[s] \n",
        "      except KeyError:\n",
        "        raise ValueError(f'state {s} does not exist')\n",
        "      try:\n",
        "        a_index = self.__action_index[a]\n",
        "      except KeyError:\n",
        "        raise ValueError(f'action {a} does not exist')\n",
        "      if a_index in  self.__allowed_actions[s_index]:\n",
        "        raise ValueError(f'action {a} repeated for state {s}')\n",
        "      self.__allowed_actions[s_index].add(a_index)\n",
        "      if len(tp) != self.__nstates:\n",
        "        raise ValueError(f'transition probability array has wrong size for state {s}, action {a}')\n",
        "      self.__p[s_index, a_index, :] = tp\n",
        "      self.__r[a_index, s_index] = r\n",
        "\n",
        "  def policy_value(self, policy, gamma):\n",
        "    b = np.array([policy[i, :].dot(self.__r[:, i]) for i in range(self.__nstates)])\n",
        "    M = gamma * np.array([\n",
        "        [policy[i,:].dot(self.__p[i, :, j]) for j in range(self.__nstates)]\n",
        "        for i in range(self.__nstates)\n",
        "    ])\n",
        "    V_pi = np.linalg.solve(np.eye(self.__nstates) - M, b)\n",
        "    return V_pi\n",
        " \n",
        "  def policy_iteration(self, gamma, tol=1e-5, maxiter=100):\n",
        "    policy = np.random.randint(self.__nstates, size=self.__nactions)\n",
        "    b = \n",
        "    success = True\n",
        "    for _ in range(maxiter):\n",
        "\n",
        "\n",
        "    \n",
        "  def pretty_print(self):\n",
        "    for s_index, s in enumerate(self.__states):\n",
        "      print(f'State {s}:')\n",
        "      for a_index, a in enumerate(self.__actions):\n",
        "        print(f'  Action {a}. Reward: {self.__r[a_index, s_index]};',\n",
        "              f'Transition probabilities: {self.__p[s_index, a_index, :]}')\n"
      ],
      "metadata": {
        "id": "6WQtb9gfKYD0"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tiny_robot_states = [1, 2, 3, 4]\n",
        "tiny_robot_actions = ['A', 'B']\n",
        "tiny_robot_data = {\n",
        "  (1, 'A'): (1, [1/3, 2/3,   0,   0]),\n",
        "  (1, 'B'): (4, [  0, 1/2,   0, 1/2]),\n",
        "  (2, 'A'): (2, [  0, 1/3, 2/3,   0]),\n",
        "  (2, 'B'): (3, [1/2,   0, 1/2,   0]),\n",
        "  (3, 'A'): (3, [  0,   0, 1/3, 2/3]),\n",
        "  (3, 'B'): (2, [  0, 1/2,   0, 1/2]),\n",
        "  (4, 'A'): (4, [2/3,   0,   0, 1/3]),\n",
        "  (4, 'B'): (1, [1/2,   0, 1/2,   0]),\n",
        "}\n",
        "tiny_robot_mdp = MDP(tiny_robot_states, tiny_robot_actions, tiny_robot_data)"
      ],
      "metadata": {
        "id": "L4WFHfjbgRip"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tiny_robot_mdp.pretty_print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecH1vdbavCbN",
        "outputId": "0a70c2ec-7cb4-4592-d057-1492dac1ef43"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State 1:\n",
            "  Action A. Reward: 1.0; Transition probabilities: [0.33333333 0.66666667 0.         0.        ]\n",
            "  Action B. Reward: 4.0; Transition probabilities: [0.  0.5 0.  0.5]\n",
            "State 2:\n",
            "  Action A. Reward: 2.0; Transition probabilities: [0.         0.33333333 0.66666667 0.        ]\n",
            "  Action B. Reward: 3.0; Transition probabilities: [0.5 0.  0.5 0. ]\n",
            "State 3:\n",
            "  Action A. Reward: 3.0; Transition probabilities: [0.         0.         0.33333333 0.66666667]\n",
            "  Action B. Reward: 2.0; Transition probabilities: [0.  0.5 0.  0.5]\n",
            "State 4:\n",
            "  Action A. Reward: 4.0; Transition probabilities: [0.66666667 0.         0.         0.33333333]\n",
            "  Action B. Reward: 1.0; Transition probabilities: [0.5 0.  0.5 0. ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For now, we model a policy as an array. Rows are state indexes, columns are action indexes."
      ],
      "metadata": {
        "id": "bVj0GeTVeh4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy = np.array([\n",
        "    [1/2, 1/2],\n",
        "    [1/4, 3/4],\n",
        "    [2/3, 1/3],\n",
        "    [0, 1]\n",
        "]);\n",
        "policy   "
      ],
      "metadata": {
        "id": "paWOQcvrvajE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84c23724-4faf-4ba8-a197-0979b620b245"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5       , 0.5       ],\n",
              "       [0.25      , 0.75      ],\n",
              "       [0.66666667, 0.33333333],\n",
              "       [0.        , 1.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tiny_robot_mdp.policy_value(policy, 0.8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjMuWc9h-Tvq",
        "outputId": "c877e6ca-af53-43ed-a494-f0f0d0a74f5c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([11.61247481, 11.87213775, 11.18519875, 10.11906943])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4La9dgIyip5O",
        "outputId": "b8372c93-3ab5-4356-ee53-c77a045f8a18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<__main__.MDP object at 0x7fc808491ca0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZjzyZfK78_vv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}