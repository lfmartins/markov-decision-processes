{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3JieFZIesic0JlEGDVTvX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lfmartins/markov-decision-processes/blob/main/Markov_Decision_Processes_Numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update -y\n",
        "!sudo apt-get install python3.11\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 2\n",
        "!python --version"
      ],
      "metadata": {
        "id": "2ZL3ivUevlBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZSwezsRuKRdP"
      },
      "outputs": [],
      "source": [
        "import numpy as numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "We want to build a framework for MDPs. The basic ingredients are:\n",
        "\n",
        "- A set of *states* $\\mathscr{S}$. We assume that $\\mathscr{S}$ is finite and let $N=|\\mathscr{S}|$\n",
        "- A set of *actions* $\\mathscr{A}$. We assume that $\\mathscr{A}$ is finite.\n",
        "- For each $s\\in\\mathscr{S}$, a set of *admissible actions for state $s$*, $\\mathscr{A}_s\\subset\\mathscr{A}$.\n",
        "- For each $s,s'\\in\\mathscr{S}$ and $a\\in\\mathscr{A}_s$, a number $p(s'\\,|\\,s,a)\\in[0,1]$.\n",
        "- For each $s\\in\\mathscr{S}$ and $a\\in\\mathscr{A}$, a number $r(s,a)$. We let the set of possible reward be $\\mathscr{R}$. We also assume this set to be finite.\n",
        "\n",
        "We interpret $p(s'\\,|\\,s,a)$ as the probability that, if the agent is in state $s$ and action $a$ is chosen, the agent will next transition to state $s'$. We require that, for each $s$, $a$ we have:\n",
        "$$\n",
        "\\sum_{s'\\in\\mathscr{S}}p(s'\\,|\\,s,a)=1\n",
        "$$\n",
        "\n",
        "The function $r(s,a)$ represents a *reward* received by the agent for visiting state $s$ and choosing action $a$. We can easily extend the definition to cover for randomized rewards.\n",
        "\n",
        "We will only consider infinite-horizon problems, so we only have to deal with stationary policies. A *randomized policy* is a specification, for every $s\\in\\mathscr{S}$ and $a\\in\\mathscr{A}$ of a number $\\pi(a\\,|\\,s)$, interpreted as the probability that action $a$ is chosen when in state $s$. We require:\n",
        "$$\n",
        "0\\le\\pi(a\\,|\\,s)\\le 1,\\quad \\sum_{a\\in\\mathscr{A_s}}\\pi(a\\,|\\,s)=1\n",
        "$$\n",
        "If, for all $s\\in\\mathscr{S}$ there is an $a=a(s)\\in\\mathscr{A}_s$ such that $\\pi(a\\,|\\,s)=1$, we say that $\\pi$ is *deterministic*.\n",
        "\n",
        "The *transition probability matrix* associated to a policy $\\pi$ is a $N\\times N$ matrix $Q$ defined by:\n",
        "$$\n",
        "Q_{ss'}\\sum_{a\\in\\mathscr{A}_s}\\pi(a\\,|\\,s)p(s'\\,|\\,s,a)\n",
        "$$\n",
        "It is easy to see that this is a stochastic matrix. Thus, if $\\pi_0$ is a probability distribution on $\\mathscr{S}$, there is a Markov chain $\\{(S_t,A_t,R_t)\\}_{t\\ge 0}\\subset \\mathscr{S}\\times\\mathscr{A}\\times\\mathscr{R}$ such that:\n",
        "\n",
        "- $P_{\\pi,\\pi_0}\\left[S_t=s'\\,|\\,S_{t-1}=s, A_{t-1}=a\\right]=p(s'\\,|\\,s,a)$\n",
        "- $P_{\\pi,\\pi_0}\\left[A_t=a\\,|\\,S_t=s\\right]=\\pi(a\\,|\\,s)$ if $a\\in\\mathscr{A}_t$.\n",
        "- $R_t=r(S_t,A_t)$\n",
        "\n",
        "In general, the initial distribution will not be relevant in the formulas below, so we will write simply $P_\\pi$ to the probability measure corresponding to the Markov chain. We denote the corresponding expected value by $E_\\pi$.\n",
        "\n",
        "Let $0< \\gamma < 1$ we define the *value function* of policy $\\pi$ as:\n",
        "$$\n",
        "V_\\pi(s)=E_\\pi\\left[\\sum_{t=0}^\\infty\\gamma^tR_t\\,|\\,S_0=s\\right]\n",
        "$$\n",
        "\n",
        "This value function is a solution to *Bellman's Equation*:\n",
        "$$\n",
        "V_\\pi=\\sum_{a\\in\\mathscr{A}_s}\\pi(a\\,|\\,s)\\left[r(a,s)+\\sum_{s'\\in\\mathscr{S}}\\gamma^tp(s'\\,|\\,s,a)V_\\pi(s)\\right]\n",
        "$$\n",
        "The sum is finite since $\\gamma$ is in the interval $(0,1)$.\n",
        "\n",
        "The *optimal value function* is defined as:\n",
        "$$\n",
        "V(s)=\\max\\left\\{V_\\pi(s)\\,|\\,\\text{$\\pi$ is a randomized policy}\\right\\}\n",
        "$$\n",
        "\n",
        "Bellman's Equation for the optimal value function is:\n",
        "$$\n",
        "V(s)=\\max_{a\\in\\mathscr{A}_s}\\left\\{\\pi(a\\,|\\,s)\\left[r(a,s)+\\sum_{s'\\in\\mathscr{S}}\\gamma^tp(s'\\,|\\,s,a)\\right]V(s)\\right\\}\n",
        "$$"
      ],
      "metadata": {
        "id": "ESbj5F36KazZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Structures\n",
        "\n",
        "We need to store three things:\n",
        "\n",
        "- The transition probabilities $p(s',s,a)$\n",
        "- The rewards $r(s,a)$\n",
        "- The policy\n",
        "\n",
        "A Markov Decision Process is represented by an object in the class `MDP`. This class records all states and actions. States and actions have unique IDs that are mapped to an integer (according to the order by which they are created). The state and action IDs can be any hashable object.\n",
        "\n",
        "State IDs are stored in a list `__states`, and action IDs are stored in a list `__actions`. States and actions are added to these lists in the order they are created.\n",
        "\n",
        "For storing the transition probabilities, we use a dictionary `__tp`. The keys in this dictionary are state IDs. For a state with ID `sid`, the dictionary entry `__tp[sid]` is itself a dictionary. The keys for this dictionary are action IDs. For each action, this dictionary points to an array containing the transition probabilities.\n",
        "\n",
        "Notice that there is some waste in this description, and in the future we may move to a more sparse representation.\n"
      ],
      "metadata": {
        "id": "XsR5ry1ditiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import typing\n",
        "class MDP(object):\n",
        "  def __init__(self, states=[]):\n",
        "    __states = []\n",
        "    __actions = []\n",
        "    __tp = {}\n",
        "    __reward = {}\n",
        "    for sid in states:\n",
        "      self.add_state(sid)\n",
        "\n",
        "  def add_state(self, state):\n",
        "    if state_id in self.__tp:\n",
        "      raise ValueError(f'state {state} already exists in this MDP')\n",
        "      self.__states.append(state)\n",
        "      self.__tp[state] = {}\n",
        "\n",
        "  def add_action(self, state_id, action_) "
      ],
      "metadata": {
        "id": "6WQtb9gfKYD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.version"
      ],
      "metadata": {
        "id": "6wtnbprnKXWO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "7e66cd75-924a-4780-cd2e-406da644accf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.8.10 (default, Nov 14 2022, 12:59:47) \\n[GCC 9.4.0]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecH1vdbavCbN",
        "outputId": "3b74881e-a954-4bf3-9a40-cd8d0ad30296"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "paWOQcvrvajE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}